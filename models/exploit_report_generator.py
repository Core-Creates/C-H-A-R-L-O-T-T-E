import torch
import torch.nn as nn

class ExploitReportGenerator(nn.Module):
    """
    Simple LSTM-based generative model to create textual exploit reports
    from CVE numerical features.

    Inputs:
        - features: tensor of shape (batch_size, feature_dim), e.g., CVE scores
        - target_seq: tokenized sequences of exploit reports during training
        - teacher_forcing_ratio: probability to use ground-truth tokens during training

    Output:
        - logits over vocabulary for each token in the sequence

    Note:
        This is a foundational model. Training on a dataset of (features, report text)
        pairs is required before it can generate meaningful exploit reports.
    """

    def __init__(self, feature_dim=5, embedding_dim=128, hidden_dim=256, vocab_size=1000):
        super(ExploitReportGenerator, self).__init__()

        # Encoder: project input features into decoder hidden space
        self.encoder_fc = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.ReLU(),
            nn.Linear(64, hidden_dim)
        )

        # Embedding layer to convert token indices to vectors
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # LSTM decoder to generate token sequences
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)

        # Output layer to produce logits for each vocabulary token
        self.fc_out = nn.Linear(hidden_dim, vocab_size)

    def forward(self, features, target_seq, teacher_forcing_ratio=0.5):
        batch_size, seq_len = target_seq.size()

        # Encode CVE features
        encoder_output = self.encoder_fc(features)
        hidden = encoder_output.unsqueeze(0)  # Initialize hidden state (num_layers=1)
        cell = torch.zeros_like(hidden)       # Initialize cell state as zeros

        embeddings = self.embedding(target_seq)

        outputs = torch.zeros(batch_size, seq_len, self.fc_out.out_features).to(features.device)

        # Decoder input starts with first token (usually <SOS>)
        input_token = embeddings[:, 0, :].unsqueeze(1)

        for t in range(1, seq_len):
            output, (hidden, cell) = self.lstm(input_token, (hidden, cell))
            prediction = self.fc_out(output.squeeze(1))
            outputs[:, t, :] = prediction

            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            next_input_token = target_seq[:, t] if teacher_force else prediction.argmax(1)
            input_token = self.embedding(next_input_token).unsqueeze(1)

        return outputs